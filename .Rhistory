overall_params$d_observation_period < overall_params$buffer_steps +
overall_params$d_min_period) {
stop(paste0(
'Inconsistent choice of parameters: at least one change in the period of observation ',
'(`overall_params$change_within_observation` is `TRUE`) but the duration of observation (',
overall_params$d_observation_period, ') is shorter than the minimum period duration (',
overall_params$d_min_period, ') plus the buffer (', overall_params$buffer_steps, ').',
'Please revise.')
)
}
if (overall_params$select_interesting_scenarios & is.null(overall_params$interesting_scenarios)) {
stop('You selected to apply detection only to interesting scenarios but didn\'t provide any.')
}
# Colors
vega_standard_palette <- c('#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b',
'#e377c2', '#7f7f7f', '#bcbd22', '#17becf')
major_minor_colors <- c(major=vega_standard_palette[1], minor=vega_standard_palette[2])
score_colors <- colorRampPalette(brewer.pal(9,'Pastel1'))(length(overall_params$score_types))
names(score_colors) <- overall_params$score_types
algo_colors <- colorRampPalette(brewer.pal(12,'Set3'))(length(overall_params$detect_algos))
names(algo_colors) <- overall_params$detect_algos
# Trend combinations
trend_combinations <- lapply(
1:(overall_params$n_max_change+1),
function (np) {
x <- expand.grid(
as_tibble(
replicate(np, names(overall_params$period_trends)),
.name_repair = 'minimal'
),
stringsAsFactors=F
) %>%
as_tibble() %>%
bind_cols(
as_tibble(
t(replicate(overall_params$n_max_change+1-np, as.character(NA))),
.name_repair = 'minimal'
)
)
names(x) <- paste0('trend_period_', 1:(overall_params$n_max_change+1))
x
}
)
trend_combinations <- bind_rows(trend_combinations)
View(trend_combinations)
# Remove combinations with same trends before and after a change in trend, as these cases are
# already covered in scenarios with one less trend change.
trend_cols <- names(trend_combinations)[names(trend_combinations)!='n_periods']
if (ncol(trend_combinations)>=2) {
id_remove_trendcombination <- c()
for (j in 1:(ncol(trend_combinations)-1)) {
id_remove_trendcombination <- c(
id_remove_trendcombination,
which(trend_combinations[,j+1] == trend_combinations[,j])
)
}
trend_combinations <- trend_combinations[-id_remove_trendcombination,]
}
trend_combinations$n_periods <- sapply(
1:nrow(trend_combinations),
function (i) as.integer(overall_params$n_max_change+1-sum(is.na(trend_combinations[i,])))
)
View(trend_combinations)
View(si_param)
View(si_distribution)
# Possible positions of trend changes, i.e. possible first steps of each trend period.
last_start <- overall_params$n_sim_steps - overall_params$d_min_period + 1
period_starts <- lapply(
1:(overall_params$n_max_change),
function (nc) {
x <- expand.grid(
as_tibble(
replicate(nc, 1:last_start),
.name_repair = 'minimal'
),
stringsAsFactors=F
) %>%
as_tibble() %>%
bind_cols(
as_tibble(
t(replicate(overall_params$n_max_change-nc, as.integer(NA))),
.name_repair = 'minimal'
)
)
names(x) <- paste0('period_start_', 2:(overall_params$n_max_change+1))
x <- bind_cols(tibble(period_start_1 = 1L), x)
x
}
)
pstart_1p <- c(1L, rep(as.integer(NA), overall_params$n_max_change))
names(pstart_1p) <- paste0('period_start_', 1:(overall_params$n_max_change+1))
period_starts <- bind_rows(pstart_1p, period_starts)
# Remove trend periods shorter than the minimum duration
if (ncol(period_starts)>=2) {
id_remove_changeposition <- c()
for (j in 1:(ncol(period_starts)-1)) {
id_remove_changeposition <- c(
id_remove_changeposition,
which(period_starts[,j+1] - period_starts[,j] < overall_params$d_min_period)
)
}
period_starts <- period_starts[-id_remove_changeposition,]
}
# If so desired, remove scenarios with at least one change trend but none in the observation period.
if (overall_params$change_within_observation) {
id_remove_not_obs <- c()
for (i in 1:nrow(period_starts)) {
maxchange <- max(period_starts[i,], na.rm=T)
np <- ncol(period_starts)-sum(is.na(period_starts[i,]))
if (np > 1 &
maxchange < overall_params$n_sim_steps - overall_params$d_observation_period +
overall_params$buffer_steps + 1) {
id_remove_not_obs <- c(id_remove_not_obs, i)
}
}
period_starts <- period_starts[-id_remove_not_obs,]
}
period_starts$n_periods <- sapply(
1:nrow(period_starts),
function (i) as.integer(overall_params$n_max_change+1-sum(is.na(period_starts[i,])))
)
period_starts <- period_starts %>% mutate(id_start_comb=row_number())
period_starts <- period_starts %>%
pivot_longer(cols=-c(n_periods,id_start_comb), names_to='id_period', values_to='period_start') %>%
mutate(id_period=as.integer(gsub('period_start_', '', id_period))) %>%
filter(!is.na(period_start))
# All combinations of possible parameter values
scenarios <- expand.grid(
sim_method = overall_params$sim_methods,
initial_level = overall_params$initial_levels,
n_periods = 1:(overall_params$n_max_change+1),
stringsAsFactors=F
) %>%
as_tibble() %>%
full_join(trend_combinations, by='n_periods') %>%
mutate(id_scenario=row_number()) %>%
pivot_longer(cols=contains('trend_period_'), names_to='id_period', values_to='trend') %>%
mutate(id_period=as.integer(gsub('trend_period_', '', id_period))) %>%
filter(!is.na(trend)) %>%
full_join(period_starts, by=c('n_periods', 'id_period')) %>%
unique() %>%
dplyr::select(id_scenario, sim_method, initial_level, n_periods, id_period, trend, id_start_comb,
period_start)
# Annotate scenarios with interesting ones
scenarios$interesting <- as.character(NA)
if (!is.null(overall_params$interesting_scenarios)) {
for (isc in names(overall_params$interesting_scenarios)) {
sm <- overall_params$interesting_scenarios[[isc]]$sim_method
il <- overall_params$interesting_scenarios[[isc]]$initial_level
np <- overall_params$interesting_scenarios[[isc]]$n_periods
trends <- overall_params$interesting_scenarios[[isc]]$trend
scenarios <- scenarios %>%
mutate(interesting=replace(
interesting,
sim_method %in% sm & initial_level==il & n_periods==np,
isc
))
for (ids in scenarios %>% filter(!is.na(interesting)) %>% pull(id_scenario) %>% unique()) {
s_trends <- scenarios %>% filter(id_scenario==ids) %>% dplyr::select(id_period, trend) %>%
unique() %>% arrange(id_period) %>% pull(trend)
if (!identical(s_trends, trends)) {
scenarios <- scenarios %>%
mutate(interesting=replace(interesting, id_scenario==ids & interesting==isc, NA))
}
}
}
}
# Look only at interesting scenarios?
if (overall_params$select_interesting_scenarios) {
scenarios <- scenarios %>% filter(!is.na(interesting))
}
saveRDS(scenarios, here(data_relative_path, 'scenarios.rds'))
if (compute_simulations) {
simulations <- NULL
# t1 <- Sys.time()
for (ids in sort(unique(scenarios$id_scenario))) {
## DEBUG
# ids <- scenarios %>% filter(interesting=='steady_state') %>% pull(id_scenario) %>% unique()
# ids <- scenarios %>% filter(interesting=='lockdown') %>% pull(id_scenario) %>% unique()
# ids <- scenarios %>% filter(interesting=='relapse') %>% pull(id_scenario) %>% unique()
# ids <- scenarios %>% filter(interesting=='flareup') %>% pull(id_scenario) %>% unique()
sm <- scenarios %>% filter(id_scenario==ids) %>% pull(sim_method) %>% unique()
istart <- scenarios %>% filter(id_scenario==ids) %>% pull(id_start_comb) %>% unique()
il <- scenarios %>% filter(id_scenario==ids) %>% pull(initial_level) %>% unique()
trends <- scenarios %>% filter(id_scenario==ids) %>% pull(trend) %>% unique()
interest <- scenarios %>% filter(id_scenario==ids) %>% pull(interesting) %>% unique()
if (sm=='project') {
# Simulate infections with branching process from `projections::project()`.
# Then to simulate under-reporting: sample from the infection count: each infection is
# reported with probability `overall_params$report_prob`.
run_offset <- 0L
for (ist in istart) {
## DEBUG
# ist <- 2
change_at <- scenarios %>%
filter(id_scenario==ids & id_start_comb==ist) %>%
pull(period_start) %>% unique()
if (length(change_at)==1) {
change_at <- NULL
} else {
change_at <- change_at[-1]
}
sim_param <- list(
n_ini=project_params$init_incidence[[il]],
R=lapply(
trends,
function(tre)
rlnorm(
overall_params$n_replicates,
log(project_params$R_means[[tre]]),
project_params$R_sd
)
),
time_change = change_at,
duration_ini = project_params$init_duration,
duration = overall_params$n_sim_steps,
n_replicates = overall_params$n_replicates,
si = si_distribution
)
project_res_list <- lapply(
1:overall_params$n_replicates,
function (i_rep) {
infect_count <- as.integer(ProjectSimulations(sim_param)[,i_rep])
as.integer(sapply(infect_count,
function (ic) rbinom(n=1, size=ic, prob=overall_params$report_prob)))
}
)
names(project_res_list) <- 1:overall_params$n_replicates
project_res <- bind_cols(project_res_list) %>% as_tibble()
project_res$interesting <- interest
project_res$id_scenario <- ids
project_res$sim_method <- sm
project_res$id_start_comb <- ist
project_res$sim_step <- 1:nrow(project_res)
project_res <- project_res %>%
as_tibble() %>%
pivot_longer(cols=-c(interesting, id_scenario, sim_method, id_start_comb, sim_step),
names_to='replicate', values_to='count') %>%
mutate(
sim_run=as.integer(replicate)+run_offset,
replicate=as.integer(replicate),
class=NA) %>%
dplyr::select(interesting, id_scenario, sim_method, id_start_comb, sim_run, replicate,
sim_step, count, class) %>%
arrange(sim_run, sim_step)
run_offset <- run_offset + overall_params$n_replicates
# Classes of observations which will serve as ground truth:
# - if there is only one period, all observations are "normal"
# - all observations in the period before last are "normal"
# - if the trend stays the same, then all observations of the last period are "normal"
# - if the trend between period before last and last changes and increases
#   (to "upward" or from "downward" to "constant"), then the observations of
#   the last period are "increase"
# - respectively, if the trend changes to "downward" or from "upward" to "constant",
#   then the observations of the last period are "decrease"
if (length(trends)==1) {
project_res$class <- 'normal'
} else {
strength_t1 <- overall_params$period_trends[[trends[length(trends)-1]]]
strength_t2 <- overall_params$period_trends[[trends[length(trends)]]]
if (strength_t1 == strength_t2) {
project_res$class <- 'normal'
} else if (strength_t1 < strength_t2) {
project_res <- project_res %>%
mutate(class=ifelse(sim_step %in% 1:(tail(change_at, 1)-1), 'normal', 'increase'))
} else if (strength_t1 > strength_t2) {
project_res <- project_res %>%
mutate(class=ifelse(sim_step %in% 1:(tail(change_at, 1)-1), 'normal', 'decrease'))
} else {
stop('Something\'s wrong with the trends: "', paste0(trends, collapse='", "'), '".')
}
}
if (!all(project_res$class %in% overall_params$classes)) {
stop('Some the attributed classes "', paste(sort(unique(project_res$class)), collapse=", "),
'" are not in the expected classes "', paste(overall_params$classes, collapse=", "), '".')
}
simulations <- simulations %>% bind_rows(project_res)
}
} else {
stop('Don\'t know simulation method "', sm, '"')
}
}
# t2 <- Sys.time()
# print('Time elapsed for generating simulations:')
# print(t2-t1)
saveRDS(simulations, here(data_relative_path, 'simulations.rds'))
# In the flare-up scenario, count how many time series actually go up in the
# second period, according to 3 criteria: mean increases, median increases, or
# the 25th percentile of the second period is larger or equal to the 75th of
# the first.
sim_count_diff_flareup <- simulations %>%
filter(interesting == "flareup") %>%
dplyr::select(sim_run, count, class) %>%
group_by(sim_run, class) %>%
summarize(
mean_count = mean(count),
med_count = median(count),
q25_count = quantile(count, probs = 0.25),
q75_count = quantile(count, probs = 0.75)
) %>%
mutate(ref_count = case_when(
class == "increase" ~ q25_count,
class == "normal" ~ q75_count,
TRUE ~ as.numeric(NA)))
prop_flareup_increase <- c()
for (countq in c("mean_count", "med_count", "ref_count")) {
prop_flareup_increase_df <- sim_count_diff_flareup %>%
dplyr::select(sim_run, class, all_of(countq)) %>%
pivot_wider(names_from = class, values_from = all_of(countq)) %>%
mutate(actual_increase = increase >= normal)
prop_flareup_increase <- c(
prop_flareup_increase,
sum(prop_flareup_increase_df$actual_increase) /
nrow(prop_flareup_increase_df)
)
}
overview_relative_path <- paste0(img_relative_path, '/sim-project/overview')
dir.create(overview_relative_path, showWarnings = FALSE, recursive = TRUE)
prop_flareup_increase_file <- file(here(overview_relative_path,
"prop_flareup_increase.txt"))
open(prop_flareup_increase_file, open = "w")
cat("In the flare-up scenario, proprtion of time-series that are actually",
"going up in the second period, according to three criteria: mean increases,",
"median increases, or the 25th percentile of the second period is larger or",
"equal to the 75th of the first.", "\n",
"Rt(normal) = ", project_params$R_means[[
overall_params$interesting_scenarios$flareup$trends[1]
]], "\n",
"Rt(increase) = ", project_params$R_means[[
overall_params$interesting_scenarios$flareup$trends[2]
]], "\n",
"initial incidence = ", project_params$init_incidence[[
overall_params$interesting_scenarios$flareup$initial_level
]], "\n\n",
"mean: ", signif(prop_flareup_increase[1], digits = 4), "\n",
"median: ", signif(prop_flareup_increase[2], digits = 4), "\n",
"percentiles: ", signif(prop_flareup_increase[3], digits = 4), "\n",
sep = "",
file = prop_flareup_increase_file)
close(prop_flareup_increase_file)
} else {
simulations <- readRDS(here(data_relative_path, 'simulations.rds'))
}
View(simulations)
ClassifyCountCI <- function(cnt, ci) {
# Classify observed counts `cnt` as "decrease", "normal", "increase" if they are below, within or
# above the confidence interval `ci`.
classification <- c()
for (i in 1:length(cnt)) {
if (is.na(ci[['lowerbound']][i]) | is.na(ci[['upperbound']][i])) {
clas <- NA
} else if (cnt[i] < ci[['lowerbound']][i]) {
clas <- 'decrease'
} else if (cnt[i] > ci[['upperbound']][i]) {
clas <- 'increase'
} else {
clas <- 'normal'
}
classification <- c(classification, clas)
}
return(classification)
}
# All combinations of simulations and detection parameters leading to as many detection runs.
detection_comb <- simulations %>%
dplyr::select(interesting, id_scenario, sim_method, id_start_comb, sim_run) %>%
unique() %>%
mutate(id_detect_comb=row_number()) %>%
dplyr::select(id_detect_comb, everything())
detectmeth_alpha_comb <- expand.grid(
id_detect_comb = detection_comb$id_detect_comb,
detect_method = overall_params$detect_algos,
alpha = overall_params$detect_alpha,
stringsAsFactors=F
) %>% as_tibble()
detection_comb <- detection_comb %>%
full_join(detectmeth_alpha_comb, by='id_detect_comb') %>%
mutate(id_detect_comb=row_number())
if (compute_detections) {
detections <- NULL
asmodee_k <- NULL
scenario_seen <- c()
detect_examples <- list()
t3 <- Sys.time()
for (idc in detection_comb$id_detect_comb) {
## DEBUG
idc <- detection_comb %>% filter(interesting=='relapse' & sim_method=='project' & sim_run==1 & detect_method=='ASMODEE_optimal' & alpha==0.05) %>% pull(id_detect_comb)
idc <- detection_comb %>% filter(interesting=='relapse' & sim_method=='project' & sim_run==1 & detect_method=='NegBin' & alpha==0.05) %>% pull(id_detect_comb)
idc <- detection_comb %>% filter(interesting=='lockdown' & sim_method=='project' & sim_run==1 & detect_method=='modified_Farrington' & alpha==0.05) %>% pull(id_detect_comb)
if (round(100000*idc/nrow(detection_comb))/1000 == round(100*idc/nrow(detection_comb))) {
cat('detect: ', idc, ' / ', round(100*idc/nrow(detection_comb)), '% // ', sep='')
print(Sys.time()-t3)
}
ids <- detection_comb %>% filter(id_detect_comb==idc) %>% pull(id_scenario)
sm <- detection_comb %>% filter(id_detect_comb==idc) %>% pull(sim_method)
sr <- detection_comb %>% filter(id_detect_comb==idc) %>% pull(sim_run)
dm <- detection_comb %>% filter(id_detect_comb==idc) %>% pull(detect_method)
al <- detection_comb %>% filter(id_detect_comb==idc) %>% pull(alpha)
if (!ids %in% scenario_seen) {
scenario_seen <- c(scenario_seen, ids)
detect_examples[[ids]] <- list()
detect_examples[[ids]][[dm]] <- list()
}
count_timeseries <- simulations %>%
filter(id_scenario==ids & sim_run==sr) %>%
dplyr::select(sim_step, count)
if(grepl('ASMODEE', dm)) {
if (grepl('manual', dm)) {
asmodee_res <- asmodee(
count_timeseries %>% rename(date=sim_step),
models = asmodee_params$models,
alpha = al,
fixed_k = asmodee_params$k_manual,
method = asmodee_params$method,
uncertain = FALSE,
simulate_pi = TRUE
)
} else if (grepl('optimal', dm)) {
asmodee_res <- asmodee(
count_timeseries %>% rename(date=sim_step),
models = asmodee_params$models,
alpha = al,
max_k = asmodee_params$k_optimal_max,
method = asmodee_params$method,
uncertain = FALSE,
simulate_pi = TRUE
)
} else {
stop('Wrong ASMODEE method "', dm,
'". It should be either with manual k ("ASMODEE_manual") ',
'or optimal k ("ASMODEE_optimal").')
}
asmodee_k <- asmodee_k %>%
bind_rows(tibble(id_detect_comb=idc, detect_method=dm, alpha=al, k=asmodee_res$k))
if (sr==1 & sm=='project') {
detect_examples[[ids]][[dm]][[as.character(al)]] <- asmodee_res
}
detection_res <- asmodee_res$results %>%
as_tibble() %>%
dplyr::select(date, classification) %>%
rename(sim_step = date)
} else if (dm=='NegBin') {
negbin_trainset <- count_timeseries %>%
filter(sim_step < overall_params$n_sim_steps - overall_params$d_observation_period + 1) %>%
dplyr::select(sim_step, count)
negbin_model <- glm.nb(count~1, link='log', data=negbin_trainset)
if (al==0) {
negbin_ci <- c(0,Inf)
} else {
nb_size <- negbin_model$theta
nb_mu <- exp(negbin_model$coefficients[['(Intercept)']])
negbin_ci <- list(
lowerbound = rep(qnbinom(al/2, size=nb_size, mu=nb_mu), overall_params$n_sim_steps),
upperbound = rep(qnbinom(1-al/2, size=nb_size, mu=nb_mu), overall_params$n_sim_steps)
)
}
detection_res <- count_timeseries %>%
mutate(classification = ClassifyCountCI(count, negbin_ci)) %>%
dplyr::select(sim_step, classification)
if (sr==1 & sm=='project') {
negbin_res <- count_timeseries %>%
full_join(detection_res, by='sim_step') %>%
mutate(ci_lb=negbin_ci[['lowerbound']], ci_ub=negbin_ci[['upperbound']])
detect_examples[[ids]][[dm]][[as.character(al)]] <- negbin_res
}
} else if (dm=='modified_Farrington') {
sts <- sts(count_timeseries$count, frequency=7)
ff_out <- farringtonFlexible(sts, control=append(list(alpha=al/2), ff_control))
ff_size <- ff_out@control$mu0Vector/(ff_out@control$phiVector-1)
ff_mu <- ff_out@control$mu0Vector
ff_ci <- list(
lowerbound = c(
rep(NA, overall_params$n_sim_steps-length(ff_out@epoch)),
qnbinom(al/2, size=ff_size, mu=ff_mu)
),
upperbound = c(
rep(NA, overall_params$n_sim_steps-length(ff_out@epoch)),
qnbinom(1-al/2, size=ff_size, mu=ff_mu)
)
)
detection_res <- count_timeseries %>%
mutate(classification = ClassifyCountCI(count, ff_ci)) %>%
dplyr::select(sim_step, classification)
if (sr==1 & sm=='project') {
ff_res <- count_timeseries %>%
full_join(detection_res, by='sim_step') %>%
mutate(ci_lb=ff_ci[['lowerbound']], ci_ub=ff_ci[['upperbound']])
detect_examples[[ids]][[dm]][[as.character(al)]] <- ff_res
}
} else {
stop('Don\'t know detection algorithm "', dm, '".')
}
detection_res <- detection_res %>%
mutate(classification = as.character(classification)) %>%
filter(sim_step >= overall_params$n_sim_steps - overall_params$d_observation_period + 1) %>%
mutate(id_detect_comb=idc, id_scenario=ids, sim_method=sm, sim_run=sr,
detect_method=dm, alpha=al) %>%
left_join(simulations, by=c('id_scenario', 'sim_method', 'sim_run', 'sim_step')) %>%
dplyr::select(id_detect_comb, interesting, id_scenario, sim_method, sim_run, detect_method, alpha,
sim_step, count, classification, class)
detections <- detections %>% bind_rows(detection_res)
}
t4 <- Sys.time()
print('Time elapsed for applying detection algorithms:')
print(t4-t3)
saveRDS(asmodee_k, here(data_relative_path, 'asmodee_k.rds'))
saveRDS(detections, here(data_relative_path, 'detections.rds'))
saveRDS(detect_examples, here(data_relative_path, 'detect_examples.rds'))
} else {
asmodee_k <- readRDS(here(data_relative_path, 'asmodee_k.rds'))
detections <- readRDS(here(data_relative_path, 'detections.rds'))
detect_examples <- readRDS(here(data_relative_path, 'detect_examples.rds'))
}
